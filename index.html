<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Haofei Ma - Academic Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Haofei Ma - Academic Homepage">
<meta property="og:title" content="Haofei Ma - Academic Homepage">


  <link rel="canonical" href="https://academic.mahaofei.com/">
  <meta property="og:url" content="http://academic.mahaofei.com/">



  <meta property="og:description" content="A PhD student in The Hong Kong Polytechnic University. Focusing on robot learning and human-robot collaboration.">









<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="static/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>

    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="static/css/academicons.css">

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='static/js/latest.js' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="#about-me">About Me</a></li>
          
            <li class="masthead__menu-item"><a href="#-educations">Educations</a></li>
          
            <li class="masthead__menu-item"><a href="#-projects">Projects</a></li>
          
            <li class="masthead__menu-item"><a href="#-publications">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="#-honors-and-awards">Honors and Awards</a></li>
          
            <li class="masthead__menu-item"><a href="#-societies">Societies</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="static/picture/android-chrome-512x512.png" class="author__avatar" alt="Haofei Ma">
  </div>

  <div class="author__content">
    <h3 class="author__name">Haofei Ma</h3>
    <p class="author__bio">The Hong Kong Polytechnic University</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;">A PhD student in The Hong Kong Polytechnic University. Focusing on robot learning and human-robot collaboration.</div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Hong Kong, China</li>
      
      
      
      
        <li><a href="index1.html"><i class="fas fa-fw fa-link" aria-hidden="true"></i> Blog</a></li>
      
      
        <li><a href="email-protection.html#7f121e16133f121e171e10191a16511c1012"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
        <li><a href="https://www.researchgate.net/profile/Haofei-Ma-2"><i class="fab fa-fw fa-researchgate" aria-hidden="true"></i> ResearchGate</a></li>
      
      
      
      
      
        <li><a href="https://www.linkedin.com/in/haofei-ma-8b716429a"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
      
      
      
      
        <li><a href="https://github.com/HaofeiMa"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=9a412VgAAAAJ&hl=en"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
        <li><a href="https://orcid.org/0009-0001-5061-6191"><i class="ai ai-orcid-square ai-fw"></i> ORCID</a></li>
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="index1.html"><i class="fas fa-fw fa-link" aria-hidden="true"></i></a>
      
      
        <a href="email-protection.html#f19c90989db19c9099909e979498df929e9c"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
        <a href="https://www.researchgate.net/profile/Haofei-Ma-2"><i class="fab fa-fw fa-researchgate" aria-hidden="true"></i></a>
      
      
      
      
      
        <a href="https://www.linkedin.com/in/haofei-ma-8b716429a"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
        <a href="https://github.com/HaofeiMa"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com/citations?user=9a412VgAAAAJ&hl=en"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
        <a href="https://orcid.org/0009-0001-5061-6191"><i class="ai ai-orcid-square ai-fw"></i></a>
      
      
      
    </div>
  </div>
</div>

  
  </div>


    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            
<p><span class="anchor" id="about-me"></span></p>

<p>I am currently a PhD student in the <a href="https://www.raids.group/">RAIDS Research Group</a> at the <a href="https://www.polyu.edu.hk/en/ise/">Department of Industrial and Systems Engineering</a> of <a href="https://www.polyu.edu.hk/">The Hong Kong Polytechnic University (PolyU)</a>, mainly engaged in the research of robot learning, human-robot collaboration and robot teleoperation.</p>

<p>I earned my bachelor’s degree at School of Mechanical Engineering, Hebei University of Technology (<a href="https://mes.hebut.edu.cn/">河北工业大学机械工程学院</a>). I then pursued a master’s degree at the National Key Laboratory of Robot Technology and Systems, Harbin Institute of Technology (<a href="http://robot.hit.edu.cn/">哈尔滨工业大学机器人技术与系统国家重点实验室</a>). Now, I’m pursuing a doctoral degree at the <a href="https://www.raids.group/">RAIDS Research Group</a> in The Hong Kong Polytechnic University, under the guidance of <a href="https://www.polyu.edu.hk/ise/people/academic-staff/pai-zheng/">Prof. Pai Zheng</a>.</p>

<p>My research interest includes robot learning, human-robot collaboration. I have submitted 5 papers in SCI journals, applied 1 invention patent, authorized 3 utility model patents and 1 software copyright.</p>

<!-- 
# 🔥 News
- *2022.02*: &nbsp;🎉🎉 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;🎉🎉 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
-->

<h1 id="-educations">📖 Educations</h1>
<ul>
  <li><em>2024.09 - now</em>, <a href="https://www.polyu.edu.hk/">The Hong Kong Polytechnic University</a>, Hong Kong.</li>
  <li><em>2022.09 - 2024.06</em>, <a href="https://www.hit.edu.cn/">Harbin Institute of Technology</a>, Harbin, (Score: 90.65, rank: 13/177).</li>
  <li><em>2018.09 - 2022.06</em>, <a href="https://www.hebut.edu.cn/">Hebei University of Technology</a>, Tianjin, (GPA: 3.95/4, rank: 1/133).</li>
</ul>

<h1 id="-projects">🔍 Projects</h1>

<!-- Project -->

<h2 id="sampling-robots-in-complex-environments">Sampling Robots in Complex Environments</h2>

<div class="paper-box">
<div class="paper-box-image">
<div class="badge">2023.03 - Present</div>
<img src="static/picture/202303_Sampling_Robots_in_Complex_Environments.png" alt="sym" width="100%">
</div>
<div class="paper-box-text">

    <p>Because of the irregularities on the object’s surface and variations in lighting conditions, point cloud images obtained from a single perspective often contain significant gaps and errors at the object’s edges, which can lead to inaccurate grasping pose estimations. To tackle these challenges, this article proposes a practical robot grasping method based on 6D pose estimation and point cloud fusion. First, 6D pose estimation is conducted, utilizing the results from model point cloud and pose estimation to complete the input point cloud through ICP (Iterative Closest Point). Subsequently, the resulting complete object point cloud is used to estimate the 6D grasping pose of the object with the help of the grasping direction estimation network.</p>

  </div><div>

    <p><strong>Finished Works</strong>:</p>
    <ol>
      <li>
        <p>Establish a model-free 6D pose estimation network based on perspective matching. Construct and train the pose estimation network to achieve precise pose estimation for seen objects and a rough estimation method for untrained objects. Create a pose refinement network for untrained objects when sparse point clouds are accessible.</p>
      </li>
      <li>
        <p>Propose a point cloud fusion and filtering method based on pose estimation. This method addresses the issue of point cloud gaps at the edge of objects when the robot collects samples from a single perspective. The point cloud fusion method is designed to utilize the output of pose estimation to fill in the gaps in the input point cloud and remove noise.</p>
      </li>
      <li>
        <p>Perform grasp pose estimation using the completed point cloud. Utilizing the grasping angle prediction network and a fast search strategy, the completed point cloud enhances the stability of grasping pose generation, thereby improving the success rate of grasping and its robustness.</p>
      </li>
      <li>
        <p>Test the system in simulation and the real world using a 6DoF robot, a Realsense camera, and a laptop (ROS/Ubuntu 20.04) as the host computer.</p>
      </li>
    </ol>

  </div>
</div>

<!-- Project -->

<h2 id="autonomous-suture-robot-system-for-endoscopic-surgery">Autonomous Suture Robot System for Endoscopic Surgery</h2>

<div class="paper-box">
<div class="paper-box-image">
<div class="badge">2023.02 - 2023.07</div>
<img src="static/picture/202302_Autonomous_Suture_Operation_Robot_System_for_Endoscopic_Surgery.png" alt="sym" width="100%">
</div>
<div class="paper-box-text">

    <p>The research objective is to enhance the efficiency and accuracy of autonomous suturing in endoscopic surgery. This involves designing a 3DOF autonomous suturing instrument, developing an efficient laparoscopic surgery robot based on 3D laparoscopy, and establishing a robot system that integrates visual, expert, and navigation systems.</p>

  </div><div>

    <p><strong>Finished Works</strong>:</p>
    <ol>
      <li>
        <p>Perform the recognition and segmentation of surgical instruments and lesion tissues based on Yolov8, and determine the spatial position of the end of the surgical instruments.</p>
      </li>
      <li>
        <p>Build a vision and robotic arm fusion system, perform hand-eye calibration of cameras and robotic arms, and execute basic robot actions for visual servo.</p>
      </li>
    </ol>

  </div>
</div>

<!-- Project -->

<h2 id="rock-core-box-handling-robot">Rock Core Box Handling Robot</h2>

<div class="paper-box">
<div class="paper-box-image">
<div class="badge">2022.08 - 2023.02</div>
<img src="static/picture/202208_Rock_Core_Box_Handling_Robot.jpg" alt="sym" width="100%">
</div>
<div class="paper-box-text">

    <p>Rock Core samples obtained from drilling before oil extraction are crucial data for assessing mining value, and they are stored in dedicated core boxes. During research and analysis, it’s essential to arrange these rock core boxes neatly on-site in a designated order, one box at a time. However, these boxes are typically heavy, resulting in a high labor intensity that can impact transportation efficiency. The present invention aims to address the labor intensity associated with the current method of transporting core boxes, ultimately improving transportation efficiency.</p>

  </div><div>

    <p><strong>Finished Works</strong>:</p>
    <ol>
      <li>
        <p>Build a sensing system that utilizes infrared sensors to detect surrounding obstacles, as well as fractional laser sensors to detect the current stacking height and alignment.</p>
      </li>
      <li>
        <p>Implement rock core box instance segmentation based on Mask RCNN, combined with a depth camera to determine its corner space coordinates for visual servo during the robot handling process.</p>
      </li>
      <li>
        <p>Utilize a monocular camera and Aruco markers to achieve 2D pose adjustment of the robot in place by identifying the offset distance and angle of the Aruco marker.</p>
      </li>
    </ol>

  </div>
</div>

<!-- Project -->

<h2 id="medical-pan-tilt-control-system-based-on-binocular-vision">Medical Pan-Tilt Control System Based on Binocular Vision</h2>

<div class="paper-box">
<div class="paper-box-image">
<div class="badge">2022.01 - 2022.06</div>
<img src="static/picture/202201_Medical_Multi-DoF_Pan-Tilt_Control_System_Based_on_Binocular_Vision.jpg" alt="sym" width="100%">
</div>
<div class="paper-box-text">

    <p>In traditional medical processes, traditional Chinese medicine practitioners often need to frequently change positions and adjust their angles to view the surgical area. Additionally, during rehabilitation treatment, the camera’s range can be inadequate to cover the patient’s moving area. Current recording methods commonly used suffer from limitations and lack automation. This article introduces a multi-degree-of-freedom pan-tilt system designed to track the positions of doctors and patients.</p>

    <p><a href="https://github.com/HaofeiMa/Multi-DOF_PTZ"><strong>Project</strong></a>, <a href="https://www.youtube.com/watch?v=gDLijtdpC2w"><strong>Video</strong></a></p>

  </div><div>

    <p><strong>Finished Works</strong>:</p>
    <ol>
      <li>
        <p>Design the mechanical structure and simulate the platform’s motion, using internal toothed rotary bearings to minimize the structural size. Protective structures have been designed for all sensor components.</p>
      </li>
      <li>
        <p>Design hardware control algorithms for pan-tilt using STM32 and A4988 drivers, and establish communication between STM32 and the host computer’s ROS system for pan-tilt motion control within ROS.</p>
      </li>
      <li>
        <p>Propose a target tracking method based on HOG and SIFT feature matching, capable of short-term single target tracking while maintaining robustness to changes in the target object’s appearance.</p>
      </li>
      <li>
        <p>Construct an experimental Pan-Tilt system and conduct experiments with all proposed algorithms, using 3D printed structural components, a Realsense camera, and stepper motors.</p>
      </li>
    </ol>

  </div>
</div>

<!-- Project -->

<h2 id="spherical-environmental-information-collection-robot">Spherical Environmental Information Collection Robot</h2>

<div class="paper-box">
<div class="paper-box-image">
<div class="badge">2020.11 - 2021.05</div>
<img src="static/picture/202011_Spherical_Metamorphic_Environmental_Information_Collection_Robot.gif" alt="sym" width="50%">
<img src="static/picture/202011_Spherical_Metamorphic_Environmental_Information_Collection_Robot.png" alt="sym" width="50%">
</div>
<div class="paper-box-text">

    <p>Monitoring essential information in various hazardous environments, such as petrochemical plants, chemical plants, and disaster sites, is vital for disaster prevention and reduction. In response to this need, our team has designed a spherical environment monitoring robot system. It features a spherical metamorphic structure with multiple telescopic feet that enable the robot to rotate, roll, and achieve all-round motion with high stability. These robots utilize self-organized network communication technology to offer multiple monitoring modes, and the collected data is packaged and transmitted to the upper computer system, enabling real-time dynamic monitoring of complex environmental information over a wide area.</p>

  </div><div>

    <p><strong>Finished Works</strong>:</p>
    <ol>
      <li>Design the three-dimensional structure of the spherical robot, including the placement of controllers, sensors inside the sphere, and the spatial arrangement of leg motors.</li>
      <li>Implement control of spherical robot leg motors using STM32, calculate motion strategies, and achieve robot motion.</li>
      <li>Integrate , accelerometers, temperature and humidity sensors, gas sensors, etc., into the STM32 control system for Bluetooth-controlled robot motion and sensor data reception.</li>
    </ol>

  </div>
</div>

<h1 id="-publications">📝 Publications</h1>

<h2 id="-papers">📃 Papers</h2>

<!-- Paper 2023.12 -->
<div class="paper-box"><div class="paper-box-image"><div><div class="badge">International Journal of Advanced Manufacturing Technology</div><img src="static/picture/202409_Robotic_Grasping_Method_with_6D_Pose_Estimation_and_Point_Cloud_Fusion.png" alt="sym" width="100%"></div></div>
<div class="paper-box-text">

    <p><a href="https://doi.org/10.1007/s00170-024-14372-3"><strong>Robotic Grasping Method with 6D Pose Estimation and Point Cloud Fusion</strong></a></p>

    <p><strong>Haofei Ma</strong>, Gongcheng Wang, Hua Bai, Zhiyu Xia, Weidong Wang, Zhijiang Du</p>

    <p><a href="https://doi.org/10.1007/s00170-024-14372-3"><strong>Paper</strong></a></p>
    <ul>
      <li>A grasping pose estimation framework based on point cloud fusion and filtering is proposed, which solves the problem of sparse point clouds at object edges and facilitates more robust grasping.</li>
      <li>A novel pose estimation method based on viewpoint selection is introduced, which first uses an RGBD cam
era to reconstruct the point cloud model of the object, and then applies the principle of viewpoint selection to 
obtain the 6D pose of the object.</li>
      <li>We demonstrate that through pose estimation and point cloud fusion, this grasping framework can accurately grasp object from a single-view RGBD image, maintaining a high success rate even in cluttered scenes.</li>
    </ul>

  </div>
</div>

<!--
[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**
-->

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">IJAMT</code> <strong>Haofei Ma</strong>, Gongcheng Wang, Hua Bai, Zhiyu Xia, Weidong Wang, and Zhijiang Du. “<a href="https://doi.org/10.1007/s00170-024-14372-3"><strong>Robotic Grasping Method with 6D Pose Estimation and Point Cloud Fusion</strong></a>.” <em>The International Journal of Advanced Manufacturing Technology</em> (2024): 1-11. doi: <a href="https://doi.org/10.1007/s00170-024-14372-3">10.1007/s00170-024-14372-3</a></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">RAS</code> Gongcheng Wang, <strong>Haofei Ma</strong>, Han Wang, Pengchao Ding, Hua Bai, Wenda Xu, Weidong Wang, and Zhijiang Du. “<a href="https://doi.org/10.1016/j.robot.2023.104589"><strong>Reactive mobile manipulation based on dynamic dual-trajectory tracking</strong></a>.” <em>Robotics and Autonomous Systems</em> 172 (2024): 104589. doi: <a href="https://doi.org/10.1016/j.robot.2023.104589">10.1016/j.robot.2023.104589</a>.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">IEEE Sensors Journal</code> Zhiyu Xia, Han Wang, Yulong Men, <strong>Haofei Ma</strong>, Zexin Cao, Weidong Wang, Zhijiang Du. “<a href="https://doi.org/10.1109/JSEN.2024.3364701"><strong>Kalman Filter-based EM-optical Sensor Fusion for Bone Needle Position Tracking</strong></a>.” <em>IEEE Sensors Journal</em> (2024). doi: <a href="https://doi.org/10.1109/JSEN.2024.3364701">10.1109/JSEN.2024.3364701</a></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">RAS</code> Hua Bai, Wenrui Gao, <strong>Haofei Ma</strong>, Pengchao Ding, Gongcheng Wang, Wenda Xu, Weidong Wang, Zhijiang Du. “<a href="https://doi.org/10.1109/JSEN.2024.3364701"><strong>A study of robotic search strategy for multi-radiation sources in unknown environments</strong></a>.” <em>Robotics and Autonomous Systems</em> 169 (2023): 104529. doi: <a href="https://doi.org/10.1109/JSEN.2024.3364701">10.1109/JSEN.2024.3364701</a>.</p>
  </li>
</ul>

<h2 id="-patents">📚 Patents</h2>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Invention Patent</code> <a href="https://cprs.patentstar.com.cn/Search/Detail?ANE=9DIE1BAA2AAA8CDA8EDA9CIB9BIF9GBC9BED6BDA9HBH9IBE"><strong>A Rock Core Box Handling Robot</strong></a>, Weidong Wang, Hengbin Liang, <strong>Haofei Ma</strong>, Gongcheng Wang (CN202310547284.5, Pending)</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Utility Model Patent</code> <a href="https://cprs.patentstar.com.cn/Search/Detail?ANE=AHIA8FDA8AGA9GGE9HAA6GAA9HDD9CIC9FCA9HDC9GDF9ICF"><strong>A Spherical Metamorphic Robot and An Environmental Information Monitoring System</strong></a>, Yuhan Rao, Manhong Li, <em>Haofei Ma</em>, Yuchong Gao, Nuo Zhang, Xinyu Liu (CN202120212154.2)</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Utility Model Patent</code> <a href="https://cprs.patentstar.com.cn/Search/Detail?ANE=AIHA6AGA7BEA9DID9BIC9ICBBFIA8BDA9IBF9ICG8EEA9FDG"><strong>A Rope Driven Cleaning Robot</strong></a>, Bao Li, Manhong Li, Shuofan Li, <strong>Haofei Ma</strong>, Jidong Guo, Yuchong Gao, Yingxin Dong (CN202120545507.0)</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Utility Model Patent</code> <a href="https://cprs.patentstar.com.cn/Search/Detail?ANE=9EEB9HFD3ABA3CBA9AIB9GIF8IAA9FADBCIA9BEA9ECDAGGA"><strong>Small Ocean Vehicles Using Wave Energy to Generate Electricity</strong></a>, Yihan Gao, <strong>Haofei Ma</strong>, Shaoan Chen, Haoran Sun, Chenxi Song (CN202020078465.X)</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Software Copyright</code> <a href="https://register.ccopyright.com.cn/publicInquiry.html?type=softList&amp;registerNumber=2021SR1391064&amp;keyWord=%E9%A9%AC%E6%B5%A9%E9%A3%9E&amp;publicityType=ALL&amp;registerDateType=ALL"><strong>Identity Recognition and Infrared Temperature Measurement Control System</strong></a>, <strong>Haofei Ma</strong> (2021SR1391064)</p>
  </li>
</ul>

<h1 id="-honors-and-awards">🏆 Honors and Awards</h1>

<h2 id="-honors">🏅 Honors</h2>
<ul>
  <li><em>2024.06</em>, HeGao Scholarship in Harbin Institute of Technology</li>
  <li><em>2023.12</em>, Leading Intelligence · Wang Yanqing Scholarship</li>
  <li><em>2023.10</em>, Excellent Students of Harbin Institute of Technology</li>
  <li><em>2023.09</em>, Top Grade Scholarship in Harbin Institute of Technology</li>
  <li><em>2022.06</em>, Provincial Outstanding Graduates (Top 1%)</li>
  <li><em>2021.05</em>, Provincial Merit Student (Top 1%)</li>
  <li><em>2021.06</em>, Finely Crafted Technology Scholarship</li>
  <li><em>2021.01</em>, Outstanding Student in Hebei University of Technology</li>
  <li><em>2020.12</em>, <a href="https://mp.weixin.qq.com/s/ulEa10HIwbCN9yk4mXRcVQ">National Scholarship</a> (Top 0.3%)</li>
  <li><em>2020.12</em>, Excellent Student Cadre in Hebei University of Technology</li>
  <li><em>2020.12</em>, The First Prize Scholarship in Hebei University of Technology</li>
</ul>

<h2 id="-competitions">🎏 Competitions</h2>
<ul>
  <li><em>2021.09</em>, “Internet +” Innovation and Entrepreneurship Competition Provincial Silver Award .</li>
  <li><em>2021.08</em>, E-commerce “Innovation, Creativity, and Entrepreneurship” Challenge Provincial Third Prize.</li>
  <li><em>2021.07</em>, Zhou Peiyuan Mechanics Competition Provincial Second Prize and National Excellence Award.</li>
  <li><em>2020.12</em>, Hebei Province College Robot Competition Seconda Prize.</li>
  <li><em>2020.10</em>, Electronic Design Competition Provincial Second Prize.</li>
  <li><em>2020.10</em>, iCAN International Innovation and Entrepreneurship Competition Provincial Second Prize.</li>
  <li><em>2020.09</em>, Mechanical Innovation Design Competition Provincial Second Prize.</li>
  <li><em>2019.12</em>, Mathematics Competition Provincial First Prize.</li>
  <li><em>2019.09</em>, iCAN International Innovation and Entrepreneurship Competition Provincial Third Prize.</li>
  <li><em>2019.06</em>, Mathematics Competition Provincial Second Prize.</li>
</ul>

<h1 id="-societies">💼 Societies</h1>

<ul>
  <li><em>2023.02 - 2023.07</em>, School Office Assistant in Harbin Institute of Technology.</li>
  <li><em>2022.02 - 2022.06</em>, Class Leader in Hebei University of Technology.</li>
  <li><em>2019.09 - 2022.06</em>, Class Study Monitor in Hebei University of Technology.</li>
  <li><em>2020.09 - 2021.08</em>, Director of Haier Key Maker-Lab in Hebei University of Technology.</li>
</ul>

<!-- 
# 💬 Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)
-->

<!-- 
# 💻 Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.
-->

          </section>
        </div>
      </article>
    </div>

    <script data-cfasync="false" src="static/js/email-decode.min.js"></script><script src="static/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://raw.githubusercontent.com/HaofeiMa/academic-page/'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  	<a data-href="https://smalltool.github.io/" style="display:none;">扒站工具</a>
</body>
</html>
